{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession, Row,Column\n",
    "from pyspark.sql import HiveContext,SQLContext\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "# from pyspark.sql.types import StructField, StringType, IntegerType, StructType, BooleanType\n",
    "# from pyspark.sql.functions import (countDistinct, \n",
    "#                                    avg, \n",
    "#                                    stddev,\n",
    "#                                    initcap,\n",
    "#                                    abs,\n",
    "#                                    min, \n",
    "#                                    max, \n",
    "#                                    mean, \n",
    "#                                    format_number,\n",
    "#                                    dayofmonth,\n",
    "#                                     hour,\n",
    "#                                    dayofyear,\n",
    "#                                    month,                                                 \n",
    "#                                    year,\n",
    "#                                    weekofyear,\n",
    "#                                    format_number,\n",
    "#                                    date_add,date_sub,date_trunc,datediff,add_months,last_day,current_date,\n",
    "#                                    lower,\n",
    "#                                    upper,\n",
    "#                                    udf,\n",
    "#                                    col,\n",
    "#                                    when,\n",
    "#                                    shuffle,\n",
    "#                                    array,\n",
    "#                                    round,\n",
    "#                                    to_date,\n",
    "#                                    udf,\n",
    "#                                    date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark module needed to update the spark Dataframe\n",
    "\n",
    "- SQLContext\n",
    "- HiveContext\n",
    "- functions from pyspark sql\n",
    "\n",
    "# Connect to Hive Database\n",
    "    # HiveContext.sql('use <database>')\n",
    "    \n",
    "# Read table from hive\n",
    "\n",
    "    # df = hive Context.sql\n",
    "    # df.show()\n",
    "    \n",
    "# check null in column an replace with 0\n",
    "    \n",
    "    # df = df.withcolumn('newcol', F.when(F.col('col1').isNull(),0).otherwise(F.col('col1')))\n",
    "\n",
    "# Use equality to update column values\n",
    "\n",
    "    # df = df.withColumn('newcol', F.when(F.col('col1')==0, 100).otherwise(F.col('col1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName('pepdep').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                #### two test files to join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pep = spark.read.csv(\"/Users/naderhashemi/jose_pyspark/sparkFiles/pep.csv\", inferSchema=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept = spark.read.csv('/Users/naderhashemi/jose_pyspark/sparkFiles/dept.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(df_pep.columns):\n",
    "    print(i,'-',j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pep.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    \n",
    "# df = df1.join(df2, on=['key'], how='inner')\n",
    "\n",
    "join_df = df_pep.join(df_dept, on=['id'], how='left')\n",
    "type(join_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update column 'income' to be all positive number\n",
    "# Update column 'lastName' to be proper case\n",
    "\n",
    "\n",
    "inc_df = join_df.withColumn('income',(F.abs(join_df['income']))) \\\n",
    "                .withColumn('lastName',(F.initcap(join_df['lastName'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_df.createOrReplaceTempView('j_sql')\n",
    "spark.sql(\"select dep, mean(income) as avg_income from j_sql group by 1 order by dep\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column 'hi_income' that shows if income of each employee is greater or less than average of the \n",
    "# each group of the department\n",
    "spark.sql(\"select concat(firstName,' ',lastName) as fullName \\\n",
    "          ,a.income \\\n",
    "          ,b.avg_income \\\n",
    "          ,case when a.income > b.avg_income then 'above_avg' \\\n",
    "                else 'below_avg' \\\n",
    "                end as hi_income \\\n",
    "          from j_sql as a \\\n",
    "          left join \\\n",
    "          (select dep, mean(income) as avg_income \\\n",
    "           from j_sql group by 1 order by dep) as b \\\n",
    "          on a.dep=b.dep\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                        # DATES in PySpark\n",
    "    \n",
    "#       to_date() will convert a string date type to a date type \n",
    "#       to_date function by default will add days as '01' if day is missing\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.appName('dds_files').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dds = spark.read.csv(\"/Users/naderhashemi/jose_pyspark/sparkFiles/DSS_by_Month_CY_2012_2021.csv\" \\\n",
    "                     , inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dds.select(F.countDistinct(\"program\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroDF = dds.withColumn('zerorow',F.when(F.col(\"zeroCol\").isNull(),0) \\\n",
    "                       .otherwise(F.col(\"zeroCol\"))).drop('zeroCol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate 'active_recipients' column by grouping 'program' and 'year_month'\n",
    "\n",
    "count_df=zeroDF.groupBy('program','year_month').sum('zerorow','active_recipients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename colums\n",
    "\n",
    "renamecol = count_df.withColumnRenamed('sum(zerorow)','totZero') \\\n",
    "                    .withColumnRenamed('sum(active_recipients)','totActRec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'year_month' column to date type:  \n",
    "agg_df = renamecol.select('*',F.to_date(count_df['year_month'],'yyyy-MM').alias('yearmon')).drop('year_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using filter as a where clause in SQL\n",
    "\n",
    "agg_df.filter(agg_df['totZero'] != 0).select('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting all non zero 'sum(zerorow) that are not zero'\n",
    "\n",
    "agg_df.filter(agg_df['totZero'] != 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nec():\n",
    "       \n",
    "    def __init__(self,endOfMonth,begOfMonth,today,dob):\n",
    "        self.endOfMonth = endOfMonth \n",
    "        self.begOfMonth = begOfMonth\n",
    "        self.today = today\n",
    "        self.dob = dob\n",
    "    \n",
    "    @classmethod\n",
    "    def lastDayofMonth(self,col):\n",
    "        self.col = col\n",
    "        return F.last_day(self.col)\n",
    "  \n",
    "    @classmethod\n",
    "    def cap(self,cap):\n",
    "        self.cap = cap\n",
    "        return F.initcap(self.cap)\n",
    "  \n",
    "    @classmethod\n",
    "    def age(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        z = self.x - self.y\n",
    "        if z <= 9999 or z =='':\n",
    "            return 1\n",
    "        else:\n",
    "            return int(str(x-y)[:-4])\n",
    "       \n",
    "         \n",
    "      \n",
    "    @classmethod\n",
    "    def firstTwoDigit(self,p):\n",
    "        self.p = p\n",
    "        return (str(self.p)[:-4])\n",
    "\n",
    "    @classmethod\n",
    "    def noSpace(self,f):\n",
    "        self.f = f\n",
    "        n = ''\n",
    "        for i in (self.f) or '':\n",
    "            \n",
    "            if i != ' ':\n",
    "                n += i\n",
    "        return n.upper()\n",
    "    \n",
    "    @classmethod\n",
    "    def comstr(self,x):\n",
    "        self.x = x\n",
    "        z = ''\n",
    "        for i in self.x or '':\n",
    "            if i.isalpha():\n",
    "                z += i\n",
    "            \n",
    "        return z.upper()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlContext.udf.register('firstTwoDigit', nec.firstTwoDigit)\n",
    "firstTwoDig = F.udf(lambda z: nec.firstTwoDigit(z),T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoSpace = F.udf(lambda x: nec.noSpace(x),T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewAge = F.udf(lambda x,y: nec.age(x,y),T.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nec.age(20210410,19590527)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st='no shit !'\n",
    "nec.noSpace(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = F.udf(lambda t: nec.comstr(t),T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nec.comstr('997sh\\!*##it w98o\\rks !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.select(replace('program'),'*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.select(NoSpace('program'),'*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nec.age(30000,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nec.firstTwoDigit(450000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.select('*',nec.lastDayofMonth('yearmon')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove spaces from 'program' column by udf function \n",
    "spark.udf.register(\"noS\", nec.noSpace,T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.createOrReplaceTempView(\"nsql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=spark.sql(\"select * ,noS(program) as programNoSpace from nsql\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                # Date functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#            date_truc converts date to datetime type\n",
    "\n",
    "date_df = agg_df.select('*',F.date_trunc('month','yearmon')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datediff() function returns number of days from two given dates\n",
    "\n",
    "age_df = agg_df.withColumn('daysBetweenTwoDates',(F.datediff(F.current_date(),agg_df['yearmon']))) \\\n",
    "               .select('*',F.current_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******       date_format() and cast() functions is used to convert a date to an integer **********************\n",
    "\n",
    "age_days = age_df.withColumn('today',(F.date_format(F.current_date(),'yyyyMMdd')).cast(T.IntegerType())) \\\n",
    "                             .withColumn('yearmonday',F.date_format('yearmon','yyyyMMdd').cast(T.IntegerType())) \\\n",
    "                             .select('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_days.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_days.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = age_days.withColumn('age',NewAge('today','yearmonday').alias('age')).select('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age.show()\n",
    "age.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'awesome!', 'is', 'programming']\n"
     ]
    }
   ],
   "source": [
    "# This will sort alphabetically : capitalization IS First and matters\n",
    "\n",
    "x = ['Python', 'programming', 'is', 'awesome!']\n",
    "print(sorted(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'A', 'A', 'e', 'h', 'H', 'L', 'n', 'O', 's', 's', 't', 'w']\n"
     ]
    }
   ],
   "source": [
    "# This will Sort based on the given key function, Key can be any function and Capitalization is NOT matter\n",
    "\n",
    "y = 'whatAnAssHOLe'\n",
    "print(sorted(y, key=lambda arg: arg.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = ['grduate', 'dORO', 'Si']\n",
    "print(sorted(z, key=lambda arg: len(arg), reverse=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce() uses the function called to reduce the iterable to a single value:\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "x = ['Python', 'programming', 'is', 'awesome!', 'more of the same']\n",
    "\n",
    "print(reduce(lambda val1,val2: val1+val2 , x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data coming from twitter to Kinesis or Kafka, Flume goes to pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [T.StructField('year_month', T.StringType(), True),\n",
    "               T.StructField('program', T.StringType(), True),\n",
    "               T.StructField('active_recipients', T.StringType(), True),\n",
    "               T.StructField('zeroCol', T.StringType(), True)\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = spark.read.csv(\"/Users/naderhashemi/jose_pyspark/sparkFiles/DSS_by_Month_CY_2012_2021.csv\", schema=final_struct, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "# Generate a pandas DataFrame\n",
    "# pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(mytext)\n",
    "\n",
    "# Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "# result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
